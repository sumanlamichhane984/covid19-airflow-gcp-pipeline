# ğŸ¦  COVID-19 Airflow GCP Data Pipeline

## ğŸ“Œ Overview
This project implements a production-grade data engineering pipeline using Apache Airflow (Google Cloud Composer) to ingest live COVID-19 country-level data from the Disease.sh API, store raw snapshots in Google Cloud Storage, and load a historical, partitioned dataset into BigQuery for analytics and BI dashboards.

The pipeline runs daily and maintains a full time-series history for each country.

---

## ğŸ› ï¸ Tech Stack
- Apache Airflow (Google Cloud Composer)
- Google Cloud Storage
- BigQuery
- Python 3.10
- Disease.sh COVID-19 API

---

## ğŸ§© Pipeline Architecture
![Architecture Diagram](images/covid_pipeline_diagram.png)

---

## ğŸ”„ Workflow
1. Airflow DAG triggers on a daily schedule  
2. Live COVID-19 country data is fetched from the Disease.sh API  
3. Raw NDJSON snapshots are written to Google Cloud Storage (partitioned by date)  
4. Data is loaded into a BigQuery staging table (truncated each run)  
5. A SQL MERGE upserts data into a partitioned history table  
6. BI tools (Looker / Power BI) query the history table for analytics  

---

## ğŸ“‚ Repository Structure
```text
covid19-airflow-gcp-pipeline/
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ covid_daily_pipeline.py
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sample/
â”‚       â””â”€â”€ covid_sample.ndjson
â”œâ”€â”€ images/
â”‚   â””â”€â”€ covid_pipeline_diagram.png
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â””â”€â”€ .gitignore
