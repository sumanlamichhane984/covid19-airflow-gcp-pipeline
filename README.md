# COVID-19 Airflow GCP Pipeline

## ğŸ“Œ Overview
An automated data pipeline built with Apache Airflow (Cloud Composer) that ingests live COVID-19 country-level data, stores raw snapshots in Google Cloud Storage, and loads cleaned, historical data into BigQuery for analytics and BI dashboards.

## ğŸ› ï¸ Tech Stack
- Apache Airflow (Google Cloud Composer)
- Google Cloud Storage
- BigQuery
- Python 3.10
- Disease.sh COVID-19 API

## ğŸ§© Pipeline Architecture
![Architecture Diagram](images/covid_pipeline_diagram.png)

## ğŸ”„ Workflow
1. Airflow DAG triggers on a daily schedule  
2. Live COVID-19 country data is fetched from the Disease.sh API  
3. Raw NDJSON snapshots are written to Google Cloud Storage (partitioned by date)  
4. Data is loaded into a BigQuery staging table (truncated each run)  
5. A SQL MERGE upserts data into a partitioned history table  
6. BI tools (Looker / Power BI) query the history table for analytics  

## ğŸ“‚ Repository Structure

covid19-airflow-gcp-pipeline/
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ covid_daily_pipeline.py
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sample/
â”‚       â””â”€â”€ covid_sample.ndjson
â”œâ”€â”€ images/
â”‚   â””â”€â”€ covid_pipeline_diagram.png
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â””â”€â”€ .gitignore

## ğŸš€ How to Run
1. Deploy the DAG file to a Google Cloud Composer environment  
2. Set the Airflow Variable `COMPOSER_BUCKET` to your Composer GCS bucket  
3. The DAG runs daily and loads data into BigQuery automatically  

## ğŸ“„ License
MIT License
